---
title: OLS Post
author: "alice"
date: '2020-09-10'
excerpt: "regression..."
layout: single
toc: true
categories:
  - modeling
  - regression
---

```{r global_options, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## Motivation

I realize that it’s not 1974, and that no serious data person spends any amount of time thinking about simple linear regression. Even calling it “ordinary” least squares regression implies that it’s mundane and uninteresting. But honestly, when I first learned the mechanics of OLS in an introductory Stats class, I found it incredibly insightful. It was like learning a little universe of information in one week. It blew my mind that there is so much data being collected and analyzed by super powerful computers and being passed into fancy machine learning models that can literally predict the future. And yet, when you really dig down and get to the fundamentals, the true beginning, all it really comes from is the distances between points. 

In this post, we'll explore the mechanics of ordinary least squares regression, both single- and multi-variable, using global data on life expectancy collected by the World Health Organization. We'll get down to some bare-bones concepts of regression modeling, analyze model diagnostics, compare two models, and attempt to validate the assumptions for performing linear regression in the first place. 

## Background 


- Mechanics of OLS
  - sums of squares
  - more than one predictor
  - cross validation 
- Diagnostics
  - Accuracy, F test, R^2, RMSE, t-test for every individual predictor
- Assumptions

## Data Preparation

### Data Source

We'll be using the WHO's life expectancy dataset, found on Kaggle [here](https://www.kaggle.com/kumarajarshi/life-expectancy-who). 

### Libraries

```{r libraries}
library(tidyverse)
library(skimr)
library(broom)

```


### Data Import and Tidy
 
Reading in and having a quick look at the [dataset](https://www.kaggle.com/kumarajarshi/life-expectancy-who): 
```{r import}
lifexp_df = read.csv(file = "../data/regression/who_life_expectancy.csv") %>% 
  janitor::clean_names() 

glimpse(lifexp_df)

skim(lifexp_df)

```
The dataset contains 2938 rows, spanning 22 variables. Every row represents a country and year combination - a total of 193 unique countries for every year from 2000 to 2015. We're going to limit the analysis to one year since having the same country repeated as a new row constitutes a repeated measure, which requires more sophisticated analysis than what we're doing here. I'm going to (arbitrarily) choose 2012. 

We also notice from the `glimpse()` output that we have quite a few missing values across many variables. Of the 22 variables available, we're really only going to focus on the following variables:
* `life_expectancy`: average life expectancy at birth, measured in years
* `schooling`: national average of years of formal education
* `status`: binary variable coded "Developed" or "Developing" (this will allow for some interesting stratification later on)

We will ignore the other variables for this analysis. Thus, our final dataset is as follows: 
```{r}
final_df = 
  lifexp_df %>% 
  filter(year == "2012") %>% 
  drop_na(life_expectancy, schooling, status)

```

## Analysis


### Ordinary Least Squares Regression

To visualize the math behind OLS, let's take a look at a scatterplot of life expectancy vs schooling. 

```{r}
final_df %>% 
  ggplot(aes(x = schooling, y = life_expectancy)) + 
  geom_point() +
  labs(
    title = "Life Expectancy vs Schooling"
  )

```

We have a pretty linear relationship with potentially some heteroskedasticity, which we'll talk about later on. We're going to ask R to fit a line through these data points and then we'll break down how R came up with this line. 

```{r}
model_1 = 
  lm(data = final_df, life_expectancy ~ schooling)

```
The coefficients are the "beta" terms, i.e. the parameters of the regression line: the intercept, \\(b_0 \\), estimated as 41.821 years, represents the average life expectancy in a theoretical nation where the average years of schooling was 0 years. This value is meaningless because there are no nations with this average education level and to interpret a regression line beyond the scope of the data that generated it is a cardinal sin in stats world. The slope, \\(b_1 \\), is estimated as 2.293, and represents the rate of change in life expectancy per additional year of schooling. So on average, countries with one additional year of schooling add 2.293 years their average life expectancy. 

To visualize our model:
```{r}
final_df %>% 
  ggplot(aes(x = schooling, y = life_expectancy)) + 
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(
    title = "Life Expectancy vs Schooling"
  )

```

To dissect this model, we'll need to know more about it than what`lm()` gave us, and, importantly, we want to output to be in tidy dataframe. This is where the `broom` package comes in handy. Specifically, the `augment()` function allows us to build a dataframe with all kinds of good information.

```{r}
model_1_df = 
  broom::augment(model_1)
model_1_df

```

The critical column here is the __residuals__ vector, `.resid`, i.e. the vertical distances between the points and the fitted line. If we were to look at just the data points, close one eye, and draw a line through them, we'd probably come with something close to what R came up with. But while we would use the complex machinery of our brain's pattern-recognition systems, the actual math behind the blue line is fairly straightforward. It all comes down to these residuals, which form the basis of least squares regression. In fact, when we say "least squares", we're referring to minimizing the squares of these values. Let's visualize them here using `geom_segment()`: 

```{r}
model_1_df %>% 
  ggplot(aes(x = schooling, y = life_expectancy)) + 
  geom_point() +
  geom_segment(aes(xend = schooling, yend = .fitted), color = "red") +
  geom_smooth(method = "lm", se = FALSE) +
  labs(
    title = "Life Expectancy vs Schooling"
  )

```
The red lines are the residuals and the way R computes the fitted model is by minimizing the squares of these red lines. The lines are squared to avoid...

We hardly ever are interested in just one predictor. The beauty of regression modeling is in its flexibility - we can add as many predictors to the right side of the equation as we want, and they don't need to be continuous variables. We can add categorical variables using "dummy variables" (explained [here](https://sphweb.bumc.bu.edu/otlt/MPH-Modules/PH717-QuantCore/PH717_MultipleVariableRegression/PH717_MultipleVariableRegression4.html)). There is of course a trade-off in flexibility if you add tons of predictors, and generally speaking, you should only add predictors that make theoretical sense and keep your model as simple as possible. 

It should also be clear that when you add a second predictor, you're no longer working in two-dimensional space. You need a third dimension to describe the relationship and you will no longer be fitting a regression line, but a regression plane: 

PUT PIC

### Model Diagnostics





going to jump right in and fit a linear model. Regression is different from other statistical methods in that you check assumptions after running the analysis, so we'll come back to those later. 


```{r}
summary(model_1)

```

From the summary function, we 
```{r}
broom::glance(model_1)


```

### Assumptions



## Conclusions



## Further Reading 
- https://drsimonj.svbtle.com/visualising-residuals



## References

-


[^1]: